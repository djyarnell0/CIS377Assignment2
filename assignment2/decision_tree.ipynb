{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision trees use many popular methods to figure out what values to split on. Two popular methods are gini index and entropy. Below you will code these two methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libs\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "entropy of a random variable is the average level of “information“, “surprise”, or “uncertainty” inherent in the variable’s possible outcomes.\n",
    "\n",
    "Below is the formula for entropy\n",
    "\n",
    "![entropy](images/entropy.png)\n",
    "\n",
    "Here is another formula that is the same but explains it with more detail\n",
    "\n",
    "![entropy2](images/entropy2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement entropy function\n",
    "from math import log2\n",
    "\n",
    "\n",
    "def entropy(num_positive_classes:int,num_negative_classes:int) -> float:\n",
    "    ####################################STUDENT CODE########################################\n",
    "    entropy = -1*(num_positive_classes*(log2(num_positive_classes)) + (num_negative_classes*(log2(num_negative_classes))))\n",
    "    ######################################END CODE##########################################\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nItems are not equal to 7 significant digits:\n ACTUAL: 0.88129089\n DESIRED: -24.406371956566694",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Downloads\\assignment2\\decision_tree.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Downloads/assignment2/decision_tree.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m negative_class \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Downloads/assignment2/decision_tree.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m value \u001b[39m=\u001b[39m entropy(positive_class,negative_class)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Downloads/assignment2/decision_tree.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m np\u001b[39m.\u001b[39;49mtesting\u001b[39m.\u001b[39;49massert_approx_equal(\u001b[39m0.88129089\u001b[39;49m,value)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Downloads/assignment2/decision_tree.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mpassed\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\djyar\\anaconda3\\envs\\CIS377\\lib\\site-packages\\numpy\\testing\\_private\\utils.py:698\u001b[0m, in \u001b[0;36massert_approx_equal\u001b[1;34m(actual, desired, significant, err_msg, verbose)\u001b[0m\n\u001b[0;32m    696\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m    697\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mabs(sc_desired \u001b[39m-\u001b[39m sc_actual) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mpower(\u001b[39m10.\u001b[39m, \u001b[39m-\u001b[39m(significant\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)):\n\u001b[1;32m--> 698\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(msg)\n",
      "\u001b[1;31mAssertionError\u001b[0m: \nItems are not equal to 7 significant digits:\n ACTUAL: 0.88129089\n DESIRED: -24.406371956566694"
     ]
    }
   ],
   "source": [
    "# test entropy code\n",
    "positive_class = 7\n",
    "negative_class = 3\n",
    "value = entropy(positive_class,negative_class)\n",
    "np.testing.assert_approx_equal(0.88129089,value)\n",
    "print('passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini Index\n",
    "The Gini index, Gini coefficient, or Gini impurity computes the degree of probability of a specific variable that is wrongly being classified when chosen randomly\n",
    "\n",
    "Below is the formula\n",
    "\n",
    "![gini](images/gini.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement gini index function\n",
    "def gini_index(num_positive_classes,num_negative_classes) -> float:\n",
    "    ####################################STUDENT CODE########################################\n",
    "    gini_value = 1 - (num_positive_classes*num_negative_classes)**2 \n",
    "    ######################################END CODE##########################################\n",
    "    return gini_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nItems are not equal to 7 significant digits:\n ACTUAL: 0.42\n DESIRED: -440.0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Downloads\\assignment2\\decision_tree.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Downloads/assignment2/decision_tree.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m negative_class \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Downloads/assignment2/decision_tree.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m value \u001b[39m=\u001b[39m gini_index(positive_class,negative_class)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Downloads/assignment2/decision_tree.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m np\u001b[39m.\u001b[39;49mtesting\u001b[39m.\u001b[39;49massert_approx_equal(\u001b[39m0.4200000\u001b[39;49m,value)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Downloads/assignment2/decision_tree.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mpassed\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\djyar\\anaconda3\\envs\\CIS377\\lib\\site-packages\\numpy\\testing\\_private\\utils.py:698\u001b[0m, in \u001b[0;36massert_approx_equal\u001b[1;34m(actual, desired, significant, err_msg, verbose)\u001b[0m\n\u001b[0;32m    696\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m    697\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mabs(sc_desired \u001b[39m-\u001b[39m sc_actual) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mpower(\u001b[39m10.\u001b[39m, \u001b[39m-\u001b[39m(significant\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)):\n\u001b[1;32m--> 698\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(msg)\n",
      "\u001b[1;31mAssertionError\u001b[0m: \nItems are not equal to 7 significant digits:\n ACTUAL: 0.42\n DESIRED: -440.0"
     ]
    }
   ],
   "source": [
    "# test gini index code\n",
    "positive_class = 7\n",
    "negative_class = 3\n",
    "value = gini_index(positive_class,negative_class)\n",
    "np.testing.assert_approx_equal(0.4200000,value)\n",
    "print('passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain\n",
    "Information gain can be defined as the amount of information improved in the nodes before splitting. You want to seperate the nodes based on features that give you the most information gain. In the picture below the left tree has more information gain. The classes are divided up into seperate child nodes. The tree on the left is more homogeneous and the tree on the right is more heterogeneous.\n",
    "\n",
    "![tree](images/tree.png)\n",
    "\n",
    "Information Gain is defined by\n",
    "\n",
    "![info](images/infogain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now observed how decision trees choose which values to split on. Next we will use a descision tree to predict some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loan Data\n",
    "The dataset we will be using is related to finances. The problem is the bank wants to know who they should give loans out to. They do not want to lend money to those who have defaulted on their loans. Using a decision tree algorithm we will see if we can predict who will and won't default on their loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\Downloads\\assignment2\\decision_tree.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Downloads/assignment2/decision_tree.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# load libraries needed\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Downloads/assignment2/decision_tree.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtree\u001b[39;00m \u001b[39mimport\u001b[39;00m DecisionTreeClassifier,plot_tree\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Downloads/assignment2/decision_tree.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m MinMaxScaler, LabelEncoder\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# load libraries needed\n",
    "from sklearn.tree import DecisionTreeClassifier,plot_tree\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data and clean the data\n",
    "df = None\n",
    "path = 'loan_defualter.csv'\n",
    "############################STUDENT CODE GOES HERE#########################\n",
    "# read in csv\n",
    "\n",
    "# there are three columns that are not needed\n",
    "# remove the Surname, CustomerId, and RowNumber columns from the df\n",
    "\n",
    "# There are two columns that are strings Geography and Gender.\n",
    "# Computers can not process strings easily and it's difficult to do math with strings.\n",
    "# The two most common approches to dealing with strings is One Hot Encoding and Label Encoding\n",
    "# Use label encoding to convert these two columns to integer values\n",
    "# use sklearn label encoding function for this\n",
    "\n",
    "#################################END CODE##################################\n",
    "\n",
    "assert(type(df) == pd.DataFrame)\n",
    "assert(df['Gender'].max() == 1)\n",
    "assert(df['Geography'].max() == 2)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test sets\n",
    "Data should be split in 80% train and 20% test. You should also shuffle the data. Use the sklearn function train_test_split for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_x, train_y, test_y= None,None,None,None\n",
    "############################STUDENT CODE GOES HERE#########################\n",
    "# split the data using train_test_split (set random_state arg to 42, train_size=0.8, test_size=0.2, shuffle=True)\n",
    "\n",
    "#################################END CODE##################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do Decision Trees need Scaled (Normalized) Data?\n",
    "\n",
    "Remember that normalizing your data is very important for ML algorithms. Trees are an exception in that the data does not have to be normalized. The reason for this is is essentially you are splitting the data by values of features. You are asking is feature x >= some value. However, normalizing won't hurt the tree algorithm and you should get the same results. Here are some examples of why you want to normalize your data for most types of algorithms.\n",
    "\n",
    "Some examples of algorithms where feature scaling matters are:\n",
    "\n",
    "* k-nearest neighbors with an Euclidean distance measure if want all features to contribute equally\n",
    "* k-means (see k-nearest neighbors)\n",
    "* logistic regression, SVMs, perceptrons, neural networks etc. if you are using gradient descent/ascent-based optimization, otherwise some weights will update much faster than others\n",
    "* linear discriminant analysis, principal component analysis, kernel principal component analysis since you want to find directions of maximizing the variance (under the constraints that those directions/eigenvectors/principal components are orthogonal); you want to have features on the same scale since you’d emphasize variables on “larger measurement scales” more.\n",
    "* Neural Networks because you want to optimize gradient descent and not introduce bias to the weights.\n",
    "\n",
    "For now since trees are the exception to this normalization rule we will not normalize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the decision tree on the train data train_x and train_y\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "############################STUDENT CODE GOES HERE#########################\n",
    "# fit the tree to the data\n",
    "\n",
    "#################################END CODE##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the decision tree\n",
    "# WARNING: This can take a while to run since this tree is large. took me around 1.2 minutes and I have good hardware\n",
    "# The image is hard to see in the notebook but you can save it off to look at on your own.\n",
    "# Wanted you to get a feel for how a real tree may look\n",
    "############################STUDENT CODE GOES HERE#########################\n",
    "# use the plot tree function that was imported a few cells above\n",
    "# if you want a more viewable tree you can give the function another argument of max_depth=1\n",
    "\n",
    "#################################END CODE##################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Decision Tree\n",
    "\n",
    "You now have a decision tree that has been trained on the loan dataset. What we want to know now is if the model can be used on data outside of the training set. We want to see how the model generalizes to data it has not seen before. Common metrics for this are accuracy, precision, and recall. Compute these metrics below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the decision tree is trained so now we need to evaluate the model and see how it performs on the test set\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "results_dict = {'Accuracy':0,'Precision':0,'Recall':0}\n",
    "############################STUDENT CODE GOES HERE#########################\n",
    "# use the decision tree to predict on the test data\n",
    "\n",
    "# compare the truth labels with the predicted labels for accuracy, precision, and recall\n",
    "# store the results into the dataframe\n",
    "#################################END CODE##################################\n",
    "\n",
    "assert(results_dict['Accuracy'] > 0.7)\n",
    "assert(results_dict['Precision'] > 0.38)\n",
    "assert(results_dict['Recall'] > 0.45)\n",
    "results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now succesfully trained and evaluated a decision tree. This is just the a basic decision tree. There are many things you can do to improve these metrics. One thing that is helpful is to have some more info as the model trains. There are many ways to do this. The way we are going to visualize training is with a learning curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize training a decision tree\n",
    "\n",
    "use the helper function plot_learning_curve to show you how the tree performs during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_helpers import plot_learning_curve\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "############################STUDENT CODE GOES HERE#########################\n",
    "# use the plot_learning_curve function to return a graph of your training\n",
    "# this fuction will use cross validation for getting the validation metrics\n",
    "# Hint: you can use the scoring argument and set it to a string to show a certain metric (\"accuracy\",\"precision\",\"recall\")\n",
    "# plot at least one of the metrics\n",
    "#################################END CODE##################################\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice how the training score is way higher than the validation score. This decision tree model is overfit to the training data. We need to make it perform better generalization. Decision trees are prone to overfitting. This is what you will work on in your own experiments. Congrats on training your decision tree and using some basic sklearn functions. This completes this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('CIS377')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89e1fe18640930373577b9dc71cb29905bc85bbd7fce8da78553e439df19ed47"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
